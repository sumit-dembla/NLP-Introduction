{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8dz0vSaO3-CC"
   },
   "source": [
    "# **NLP Basics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "IPHRrEZ3Kfcd",
    "outputId": "61747ad4-20d3-469f-cfb7-46d4b7644b00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alice', 'was', 'beginning', 'to', 'get', 'very', 'tired', 'of', 'sitting', 'by', 'her', 'sister', 'on', 'the', 'bank', ',', 'and', 'of', 'having', 'nothing', 'to', 'do', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Sumit\n",
      "[nltk_data]     Dembla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "sentence = \"Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do.\"\n",
    "\n",
    "sentence_tokenized = nltk.word_tokenize(sentence)\n",
    "\n",
    "print (sentence_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CutBjC094P3Q"
   },
   "source": [
    "Tokens can be received from a paragraph as well. Sometimes there is also a need to break down a paragraph into sentences. That can be done with `sent_tokenize()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "lCU42F01KvM4",
    "outputId": "8d684baa-bfa3-4dd4-b984-ad2d57a12b93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alice', 'was', 'beginning', 'to', 'get', 'very', 'tired', 'of', 'sitting', 'by', 'her', 'sister', 'on', 'the', 'bank', ',', 'and', 'of', 'having', 'nothing', 'to', 'do', ':', 'once', 'or', 'twice', 'she', 'had', 'peeped', 'into', 'the', 'book', 'her', 'sister', 'was', 'reading', ',', 'but', 'it', 'had', 'no', 'pictures', 'or', 'conversations', 'in', 'it', ',', '“', 'and', 'what', 'is', 'the', 'use', 'of', 'a', 'book', ',', '”', 'thought', 'Alice', '“', 'without', 'pictures', 'or', 'conversations', '?', '”']\n"
     ]
    }
   ],
   "source": [
    "paragraph = \"\"\"\n",
    "            Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, \n",
    "            but it had no pictures or conversations in it, “and what is the use of a book,” thought Alice “without pictures or conversations?”\n",
    "\"\"\"\n",
    "\n",
    "para_tokenized = nltk.word_tokenize(paragraph)\n",
    "\n",
    "print (para_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "wMYc3OaQ39Cj",
    "outputId": "08b08ac6-a0a0-4de4-97ce-71741f13cd00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n            As she said this she looked down at her hands, and was surprised to \\n            see that she had put on one of the Rabbit’s little white kid gloves while she was talking.', '“How can I have done that?” she thought.', '“I must be growing small again.”']\n"
     ]
    }
   ],
   "source": [
    "paragraph = \"\"\"\n",
    "            As she said this she looked down at her hands, and was surprised to \n",
    "            see that she had put on one of the Rabbit’s little white kid gloves while she was talking. “How can I have done that?” she thought. “I must be growing small again.”\n",
    "\"\"\"\n",
    "\n",
    "para_tokenized = nltk.sent_tokenize(paragraph)\n",
    "\n",
    "print (para_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OWdWRqBvORGM"
   },
   "source": [
    "# **Normalization**\n",
    "\n",
    "In this section we will learn techniques to get rid of unnecessary words from a corpus. There are different words such as \"was\" or \"can\" that don't hold much value when we don't care much about sentence structure. There is a list of common words such as \"and\", \"is\", \"or\", \"and\", \"a\", \"an\", \"the\", etc., which don't provide much information in a sentence. Usually for text processing we get rid of the words in this list. Here the `nltk` library comes in handy to do such cleaning of text. There are 21 lists of such common words supported in the `nltk` library. Since our paragraph is in English, we will use the respective list of stop words from `nltk`. Also \"As\" and \"as\" are usually considered the same word while counting frequency of words. It is good practice to either make it capital or small letters so that we can get rid of more common words that are just separated by case. Text normalization is an umbrella term for considering the following operations:\n",
    "\n",
    "- Removal of stop words\n",
    "- Stemming - abruptly removing the suffix from words\n",
    "- Converting text to upper or lower case\n",
    "- Lemmatization - replacing a token with its root word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "id": "EaBN7OL84Ejl",
    "outputId": "50f4ecf5-7ab4-47cd-890a-b9bdb94cc7d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "print tokens \n",
      "['As', 'said', 'looked', 'hands', ',', 'surprised', 'see', 'put', 'one', 'Rabbit', '’', 'little', 'white', 'kid', 'gloves', 'talking', '.', '“', 'How', 'I', 'done', '?', '”', 'thought', '.', '“', 'I', 'must', 'growing', 'small', '.', '”']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "#Download stop words from nltk library\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "tokens = nltk.word_tokenize(paragraph)\n",
    "\n",
    "clean_paragraph = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "try:\n",
    "  print (\"print tokens \")\n",
    "  print (clean_paragraph)\n",
    "except:\n",
    "  print (\"expected variable called clean_sentence\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "KyxfZjBYV7Rj",
    "outputId": "4e5741e8-87f1-4ced-ed8f-883a18dda83f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wow this is cool ! wow this is awesome.\n",
      "WOW THIS IS COOL ! WOW THIS IS AWESOME.\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Wow this is cool ! wow this is awesome.\"\n",
    "lit_sentence = sentence.lower()\n",
    "upp_sentence = sentence.upper()\n",
    "\n",
    "print (lit_sentence)\n",
    "\n",
    "print (upp_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uqYVnhF77zXn"
   },
   "source": [
    "# Stemming and Lemmatization\n",
    "\n",
    "There are various forms of words in grammer of any language such as differ (verb), different (adjective), differently (adverb), differentness (noun). Both stemming and lemmatization have the same goal of reducing inflectional forms. For example, \"different\", \"differently\", and \"differentness\" will be reduced to \"differ\". \"am\", \"is\", \"are\" will be reduced to \"be\".  Stemming usually chops off prefix/suffix from words. For example: \n",
    "\n",
    "Alice's ring has different colors. => Alice ring has differ color.\n",
    "\n",
    "Lemmatization uses morphological analysis to smoothly transform various form of words to a root form, which is called a *Lemma*. \n",
    "\n",
    "For the word sing, if you use stemming it may return \"s\" after chopping \"ing\", whereas lemmatization returns \"sing\" if the token was as verb. Porter's algorithm is one of the most common and effective algorithms for stemming. Let's see how we can leverage Porter's algorithm for stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "TjF_Q-JgZF32",
    "outputId": "45a15285-875e-4038-b351-9da4ef4f59f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of Tokens \n",
      "['There', 'might', 'be', 'some', 'sense', 'in', 'your', 'knocking', '.']\n",
      "\n",
      "\n",
      "List of stemmed words\n",
      "['there', 'might', 'be', 'some', 'sens', 'in', 'your', 'knock', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "sentence = \"There might be some sense in your knocking.\"\n",
    "\n",
    "tokens = word_tokenize(sentence)\n",
    "word_stemmed = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "print (\"List of Tokens \")\n",
    "print (tokens)\n",
    "\n",
    "print (\"\\n\")\n",
    "print (\"List of stemmed words\")\n",
    "print (word_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "iqjZ3lXdBi71",
    "outputId": "ca2b45d8-0695-41c4-ef68-31599d85aeb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "['There', 'might', 'be', 'some', 'sense', 'in', 'your', 'knocking', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('wordnet')\n",
    "\n",
    "Lemmatizer = WordNetLemmatizer()\n",
    "sentence = \"There might be some sense in your knocking.\"\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "lemma = [Lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "print (lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ykEXCGbsxxp9"
   },
   "source": [
    "# Bag-of-Words\n",
    "![](bag_of_words.png)\n",
    "We will now go through one of the basic techniques in NLP, which is quite useful to search (information retrieval and classification) similar documents from an available corpus of documents. This model is known as the bag-of-words model. It neither cares about grammar nor sequence order. Only the frequency of every word matters. Usually this will be applied after the removal of stop words because we don't want to count the frequency of words that don't provide much information and are there just for the grammar of the language. The frequency of words is used as a feature for the machine learning model. This technique is useful for elementary sentimental analysis, or for basic spam filtering because, usually, there are repetitive positive and negative words to figure out positive/negative sentiment, whereas a lot of spam uses the same words such as \"lottery\", \"win\", etc. We will use `sklearn` to divide sentences into tokens and count frequencies. The `CountVectorizer` function helps us achieve this. Once we use the function, `corpus_vectorizer.vocabulary_` is used to check the number of words (in a technical words vocabulary of corpus) in a sentence. Let's see this in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6lrXOlajEbcT"
   },
   "outputs": [],
   "source": [
    "corpus = [\"Emma was a Catholic because her mother was a Catholic, and Nory’s mother was a Catholic because her father was a Catholic, and her father was a Catholic because his mother was a Catholic, or had been.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "Ww9TIkOs83JZ",
    "outputId": "4a1805ac-a1e8-4dd8-e52b-0bd2fe32473c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus_vectorizer = CountVectorizer()\n",
    "corpus_vectorizer.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "colab_type": "code",
    "id": "sfW5E4g49yp-",
    "outputId": "7645538f-1d72-4e4d-9dfd-4c757da6f580"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'and': 0,\n",
       " 'because': 1,\n",
       " 'been': 2,\n",
       " 'catholic': 3,\n",
       " 'emma': 4,\n",
       " 'father': 5,\n",
       " 'had': 6,\n",
       " 'her': 7,\n",
       " 'his': 8,\n",
       " 'mother': 9,\n",
       " 'nory': 10,\n",
       " 'or': 11,\n",
       " 'was': 12}"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show count for each word present in corpus\n",
    "corpus_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p4N16_ByVQ7Z"
   },
   "source": [
    "We have used the `ntlk` library to get split words and get frequency. Let's understand it without using any NLP library. First, the sentence is broken down with white space and then a dictionary is created to keep track of tokens/words. `nltk` performs some more beautification such as removal of extra characters (,;.! etc). For example, see \"nory\" (above) and \"Nory's\" in the following code. `nltk` also converts all letters to lowercase so that \"WORD\" and \"word\" are considered the same word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OjSoRTyvVQJn"
   },
   "outputs": [],
   "source": [
    "corpus = \"Emma was a Catholic because her mother was a Catholic, and Nory’s mother was a Catholic because her father was a Catholic, and her father was a Catholic because his mother was a Catholic, or had been.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "2LPWVpaYVnoF",
    "outputId": "e78f692b-6a8b-4684-a6a0-c90611eba6e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Emma': 1, 'was': 6, 'a': 6, 'Catholic': 3, 'because': 3, 'her': 3, 'mother': 3, 'Catholic,': 3, 'and': 2, 'Nory’s': 1, 'father': 2, 'his': 1, 'or': 1, 'had': 1, 'been.': 1}\n"
     ]
    }
   ],
   "source": [
    "#Python code without any nlp library\n",
    "tokens = corpus.split()\n",
    "#Uncomment to see all the tokens\n",
    "#print (tokens)\n",
    "#define empty dictionary \n",
    "dic = {}\n",
    "\n",
    "#save frequency of each token in dictiory\n",
    "for token in tokens:\n",
    "  count = 1\n",
    "  if token not in dic:\n",
    "    dic[token] = count\n",
    "  else:\n",
    "    dic[token] = dic[token] + 1\n",
    "\n",
    "print (dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tXbxHYiP3xxC"
   },
   "source": [
    "# TF-IDF\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DUki5z0bGB4A"
   },
   "source": [
    "This section will help you with another feature extraction technique, which emphasizes unique words. TF-IDF stands for term-frequency inverse-document-frequency. It calculates two things: \n",
    "1. **Term Frequency** - (number of times a word _w_ appreared in the document) / (the number of words that appreared in the document)\n",
    "2. **Inverse Document Frequency** - How important the particular word is to the document. It can be derived by the following equation.\n",
    "\n",
    "  IDF(w) = log (Total number of documents / number of documents with word _w_ in it)\n",
    "\n",
    "For example, consider a corpus having 1 million documents and each document is made of 100 words. If the word 'love' appears 10 times in a document and across all the documents, the frequency of the word \"love\" is 10,000:\n",
    "\n",
    "Term Frequency = 10 / 100 = 0.1\n",
    "\n",
    "IDF = log(1000000/10000) = 2\n",
    "\n",
    "TF-IDF = 0.1 * 2 = 0.2\n",
    "\n",
    "Let's understand this concept by emulating a small corpus of documents (though the size of any corpus can be enormous in the real world). We will use `TfidVectorizer` from `sklearn` to convert a raw document in TF-IDF matrix form. To visualize the output better we have created a pandas data frame. Each row represents various words available in corpus. Each column represents document names. \"cute\" is present the same number of times in all documents across corpus, so the score is 1 for all and it's similar for other same words. \"she\" is unique across corpus so it has a higher score for `doc_1`. \"very\" is not common across all documents. For `doc_2`, \"very\" is less important in comparison to `doc_3` where it occured twice. \"she\", \"he\", and \"very\" is important for `doc_1`, `doc_2`, `doc_3`, respectively. In short, the higher the number, the more important term it is for that document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DoUWgxllGAk3"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "iqy-UJ1fFwR4",
    "outputId": "fbd617e2-049c-4217-c651-99130c44d083"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF table of corpus\n",
      "======================\n",
      "\n",
      "             doc_1     doc_2     doc_3\n",
      "angelina  0.000000  0.000000  1.693147\n",
      "cute      1.000000  1.000000  1.000000\n",
      "he        0.000000  1.693147  0.000000\n",
      "is        1.000000  1.000000  1.000000\n",
      "she       1.693147  0.000000  0.000000\n",
      "very      0.000000  1.287682  2.575364\n"
     ]
    }
   ],
   "source": [
    "doc_1 = \"She is cute\"\n",
    "doc_2 = \"He is very cute\"\n",
    "doc_3 = \"Angelina is very very cute\"\n",
    "\n",
    "corpus = [doc_1, doc_2, doc_3]\n",
    "\n",
    "corpus_preprocess = []\n",
    "\n",
    "for doc in corpus:\n",
    "  corpus_preprocess.append(doc.lower())\n",
    "\n",
    "corpus_vectorizer = TfidfVectorizer(norm=None)\n",
    "tf_idf_scores = corpus_vectorizer.fit_transform(corpus_preprocess)\n",
    "\n",
    "feature_names = corpus_vectorizer.get_feature_names()\n",
    "corpus_index = [doc for doc in corpus_preprocess]\n",
    "\n",
    "print (\"TF-IDF table of corpus\")\n",
    "print (\"======================\\n\")\n",
    "\n",
    "print(pd.DataFrame(tf_idf_scores.T.todense(), index = feature_names, columns = [\"doc_1\", \"doc_2\", \"doc_3\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NXO2uwGi0t6S"
   },
   "source": [
    "Internally, `CountVectorizer` and `TfidfTransformer` are used to calculate term frequency and inverse document frequency in `TfidfVectorizer`. For a better understanding, a breakdown is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 416
    },
    "colab_type": "code",
    "id": "uxnZRlMEFwcI",
    "outputId": "e9847d22-6b20-47d3-92da-ff4fd1f37088"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term Frequency Matrix\n",
      "=====================\n",
      "[[0 1 0 1 1 0]\n",
      " [0 1 1 1 0 1]\n",
      " [1 1 0 1 0 2]]\n",
      "=====================\n",
      "\n",
      "IDF table\n",
      "=========\n",
      "               IDF\n",
      "angelina  1.693147\n",
      "cute      1.000000\n",
      "he        1.693147\n",
      "is        1.000000\n",
      "she       1.693147\n",
      "very      1.287682\n",
      "\n",
      "TF-IDF matrix\n",
      "=============\n",
      "\n",
      "[[0.         1.         0.         1.         1.69314718 0.        ]\n",
      " [0.         1.         1.69314718 1.         0.         1.28768207]\n",
      " [1.69314718 1.         0.         1.         0.         2.57536414]]\n"
     ]
    }
   ],
   "source": [
    "#Part 1 - Calculating Term Frequencies\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "term_frequency = vectorizer.fit_transform(corpus_preprocess)\n",
    "\n",
    "#uncomment following to check feature names\n",
    "#print(vectorizer.get_feature_names())\n",
    "\n",
    "print (\"Term Frequency Matrix\")\n",
    "print (\"=====================\")\n",
    "print (term_frequency.toarray())\n",
    "print (\"=====================\")\n",
    "#Part 2 - Calculating Inverse Document Frequency\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "vectorizer = TfidfTransformer(norm=None)\n",
    "vectorizer.fit(term_frequency)\n",
    "\n",
    "idf = vectorizer.idf_\n",
    "\n",
    "df = pd.DataFrame(idf, index = feature_names, columns=['IDF'])\n",
    "print (\"\\nIDF table\")\n",
    "print (\"=========\")\n",
    "print(df)\n",
    "\n",
    "\n",
    "tf_idf_scores = vectorizer.fit_transform(term_frequency.toarray())\n",
    "\n",
    "\n",
    "print (\"\\nTF-IDF matrix\")\n",
    "print (\"=============\\n\")\n",
    "print (tf_idf_scores.toarray())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "o'reilly_NLP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
